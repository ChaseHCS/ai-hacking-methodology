# Context Jamming

`Context Jamming` works as a way to exploit an LLMs context window. Most `LLMs` work by first reading the `system prompt` as designated by the company. Then user input which is what we can control and will exploit. Here is an example of context-jamming `ChatGPT` to achieve `jailbreak`. Jailbreak is just an arbitrary term for getting the LLM to do something that it is not intended to do.
